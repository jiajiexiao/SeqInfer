{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqinfer.seq.datasets import SeqFromFileDataset\n",
    "from seqinfer.seq.transforms import Compose, KmerTokenizer, OneHotEncoder, ToTensor\n",
    "from seqinfer.seq.vocabularies import unambiguous_dna_vocabulary_dict, SpecialToken\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset, Subset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 120\n",
    "pos_seq_dataset = SeqFromFileDataset(\n",
    "    seq_file=\"pos.fasta\",\n",
    "    seq_file_fmt=\"fasta\",\n",
    "    transform_sequences=Compose(\n",
    "        [\n",
    "            KmerTokenizer(\n",
    "                k=1,\n",
    "                stride=1,\n",
    "                vocab_dict=unambiguous_dna_vocabulary_dict,\n",
    "                num_output_tokens=MAXLEN,\n",
    "                special_tokens=SpecialToken,\n",
    "            ),\n",
    "            OneHotEncoder(vocab_size=len(unambiguous_dna_vocabulary_dict) + len(SpecialToken)),\n",
    "            ToTensor(dtype=torch.float32),\n",
    "        ]\n",
    "    ),\n",
    "    targets=1,\n",
    "    transform_targets=ToTensor(torch.float32),\n",
    ")\n",
    "\n",
    "neg_seq_dataset = SeqFromFileDataset(\n",
    "    seq_file=\"neg.fasta\",\n",
    "    seq_file_fmt=\"fasta\",\n",
    "    transform_sequences=Compose(\n",
    "        [\n",
    "            KmerTokenizer(\n",
    "                k=1,\n",
    "                stride=1,\n",
    "                vocab_dict=unambiguous_dna_vocabulary_dict,\n",
    "                num_output_tokens=MAXLEN,\n",
    "                special_tokens=SpecialToken,\n",
    "            ),\n",
    "            OneHotEncoder(vocab_size=len(unambiguous_dna_vocabulary_dict) + len(SpecialToken)),\n",
    "            ToTensor(dtype=torch.float32),\n",
    "        ]\n",
    "    ),\n",
    "    targets=0,\n",
    "    transform_targets=ToTensor(torch.float32),\n",
    ")\n",
    "\n",
    "all_seq = ConcatDataset([pos_seq_dataset, neg_seq_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, torch.Size([120, 14]), torch.Size([120, 14]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_seq), all_seq[1][0].shape, all_seq[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 64\n",
    "train_val_ids, test_ids = train_test_split(range(len(all_seq)), test_size=0.2, random_state=0)\n",
    "train_ids, val_ids = train_test_split(train_val_ids, test_size=0.2, random_state=0)\n",
    "train_loader = DataLoader(Subset(all_seq, train_ids), batch_size=BATCHSIZE)\n",
    "val_loader = DataLoader(Subset(all_seq, val_ids), batch_size=BATCHSIZE)\n",
    "test_loader = DataLoader(Subset(all_seq, test_ids), batch_size=BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqinfer.infer.classifiers import LitBinaryClassifier, LitClassifier\n",
    "import lightning as L\n",
    "\n",
    "L.seed_everything(123, workers=True)\n",
    "\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwapAxes(nn.Module):\n",
    "    def __init__(self, dim0: int, dim1: int) -> None:\n",
    "        super().__init__()\n",
    "        self.dim0 = dim0\n",
    "        self.dim1 = dim1\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.swapaxes(x, self.dim0, self.dim1)\n",
    "\n",
    "\n",
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, dim: int | None = None) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "convnet_binary = nn.Sequential(\n",
    "    SwapAxes(1, 2),\n",
    "    nn.Conv1d(in_channels=14, out_channels=8, kernel_size=8),\n",
    "    nn.AvgPool1d(kernel_size=3),\n",
    "    nn.Conv1d(in_channels=8, out_channels=8, kernel_size=6),\n",
    "    nn.AvgPool1d(kernel_size=3),\n",
    "    nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3),\n",
    "    nn.AvgPool1d(kernel_size=3),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16, 1),\n",
    "    Squeeze(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitBinaryClassifier(\n",
    "    model=convnet_binary, is_output_logits=True, loss=nn.BCEWithLogitsLoss()\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15, verbose=False)\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\", mode=\"min\")\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=200,\n",
    "    # precision=\"bf16-mixed\",\n",
    "    deterministic=True,\n",
    "    # enable_checkpointing=False,\n",
    "    log_every_n_steps=5,\n",
    "    logger=logger,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/seqinfer/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/anaconda3/envs/seqinfer/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/anaconda3/envs/seqinfer/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=5). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 2/2 [00:00<00:00, 11.27it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "best_lit_model = LitBinaryClassifier.load_from_checkpoint(\n",
    "    checkpoint_callback.best_model_path, model=convnet_binary, loss=nn.BCEWithLogitsLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/seqinfer/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 31.16it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_BinaryAUROC      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.579487144947052     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_BinaryAccuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.4921875         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6922537684440613     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_BinaryAUROC     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.579487144947052    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_BinaryAccuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.4921875        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6922537684440613    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.6922537684440613,\n",
       "  'test_BinaryAccuracy': 0.4921875,\n",
       "  'test_BinaryAUROC': 0.579487144947052}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(best_lit_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 95.88it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_BinaryAUROC      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4745097756385803     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_BinaryAccuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.46875          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6922627091407776     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_BinaryAUROC     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4745097756385803    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_BinaryAccuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.46875         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6922627091407776    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.6922627091407776,\n",
       "  'test_BinaryAccuracy': 0.46875,\n",
       "  'test_BinaryAUROC': 0.4745097756385803}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(best_lit_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 17.99it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_BinaryAUROC      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.558080792427063     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_BinaryAccuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.550000011920929     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6961395740509033     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_BinaryAUROC     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.558080792427063    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_BinaryAccuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.550000011920929    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6961395740509033    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.6961395740509033,\n",
       "  'test_BinaryAccuracy': 0.550000011920929,\n",
       "  'test_BinaryAUROC': 0.558080792427063}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(best_lit_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6089)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "output = best_lit_model.model(x)\n",
    "torchmetrics.classification.BinaryAUROC()(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seqinfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
